# Об авторе

Эли́езер Шло́мо Юдко́вский — американский специалист по искусственному интеллекту, исследующий проблемы технологической сингулярности и выступающий за создание Дружественного ИИ. Сооснователь и научный сотрудник Института Сингулярности по созданию Искусственного Интеллекта Singularity Institute for Artificial Intelligence. Он — автор книги «Создание дружественного ИИ», статей «Уровни организации универсального интеллекта», «Когерентная экстраполированная воля» и «Вневременная теория принятия решений». Также две его научные статьи «Искусственный интеллект как позитивный и негативный фактор глобального риска» и «Когнитивные искажения в оценке глобальных рисков» включены статьи в сборник «Риски глобальной катастрофы» (2008) под редакцией Ника Бострома. Юдковский не обучался в вузах и является автодидактом без формального образования в области ИИ.

Юдковский исследует те конструкции ИИ, которые способны к самопониманию, самомодификации и рекурсивному самоулучшению (Seed AI), а также такие архитектуры ИИ, которые будут обладать стабильной и позитивной структурой мотивации (Дружественный искусственный интеллект). Помимо исследовательской работы, Юдковский известен своими объяснениями сложных моделей на неакадемическом языке, доступном широкому кругу читателей.

Юдковский был вместе с Робином Хансоном одним из главных авторов блога Overcoming Bias (преодоление предубеждений). В начале 2009 года он участвовал в организации блога LessWrong, нацеленного на «развитие рациональности человека и преодоление когнитивных искажений».
